---
title: "[RL] 강화학습 정리"
date: 2025-11-01
categories: [textbook-summary,Reinforcement-learning]
tags: [RL]
math: true
permalink: /rl/multi-armed-bandit-mdp/
---


## 1. 다중 선택 문제 (k-armed bandit problem)

**정의**  
k개의 서로 다른 option(action) 중 하나를 반복적으로 선택하여 일정 기간 동안 얻는 **보상의 총합의 기대값을 최대화**하는 문제.

$$
q_*(a)\mathrel{\dot{=}} \mathbb{E}[R_t|A_t=a]
$$

- $A_t$: 시간 단계 $t$에서 선택된 action  
- $R_t$: $t$에서 선택된 action에 대한 보상  
- $q_*(a)$: 임의의 action $a$가 선택되었을 때 얻는 평균 보상(value)  
- $Q_t(a)$: 시간 $t$에서 추정된 action $a$의 value  
  → $Q_t(a)$가 $q_*(a)$에 가까울수록 정확한 추정

---

### Greedy / $\epsilon$-Greedy 전략

- **Greedy action**: 각 $t$마다 추정 value가 가장 높은 action 선택  
  → *exploiting* (현재까지의 정보 활용)  
- 다른 action을 시도하는 것은 *exploring* (정보 탐색)  
- → 두 전략의 **균형(exploration vs. exploitation trade-off)** 이 중요

**행동 가치 방법(action-value method)**  
action의 value를 추정하고, 그 추정값으로부터 action을 선택하는 방법.

$$
Q_t(a) \mathrel{\dot{=}} 
\frac{
\text{시각 }t \text{ 이전에 취해진 action }a\text{에 대한 보상의 합}
}{
\text{시각 }t\text{ 이전에 action }a\text{를 취한 횟수}
}
= 
\frac{
\sum_{i=1}^{t-1} R_i\cdot\mathbb{1}_{A_i=a}
}{
\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}
}
$$

즉, **sample-average 방법**이다.

---

### Action 선택 규칙

1. **Greedy 방식**
   $$
   A_t = \underset{a}{\arg\max}\; Q_t(a)
   $$
2. **$\epsilon$-greedy 방식**  
   - 확률 $\epsilon$로 무작위 action 선택  
   - 확률 $1-\epsilon$로 greedy action 선택

---

### 10중 선택 테스트

- 행동: 10개 $(a=1,\dots,10)$  
- 각 $q_*(a) \sim \mathcal{N}(0,1^2)$  
- 실제 보상값: $R_t \sim \mathcal{N}(q_*(a),1^2)$

---

### 점증적 구현 (Incremental Implementation)

$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$

새로운 추정값 = 이전 추정값 + **step-size(학습률)** × [목표값 − 이전 추정값]

---

### 비정상 환경 (Nonstationary Problem)

Stationary 문제는 보상 분포가 시간이 지나도 변하지 않지만,  
Nonstationary 환경에서는 최신 보상에 더 큰 가중치를 줘야 함.

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

이를 전개하면:

$$
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}R_i
$$

즉, **기하급수적 최신 가중 평균 (Exponential Recency-Weighted Average)**

---

### 긍정적 초기값 (Optimistic Initial Values)

모든 방법은 초기 추정값 $Q_1(a)$에 영향을 받음 → **bias 존재**

---

### 신뢰 상한 탐색 (Upper Confidence Bound, UCB)

$$
A_t = \underset{a}{\arg\max}\; \Big[ Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}} \Big]
$$

추정값의 불확실성(탐색 필요성)을 반영하여 **탐욕적 행동 + 신뢰 상한**을 함께 고려.

---

### 경사도 다중 선택 (Gradient Bandit Algorithm)

소프트맥스 분포 기반으로 행동 선택:

$$
\text{Pr}\{A_t=a\} = 
\frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} 
\mathrel{\dot{=}} \pi_t(a)
$$

업데이트 규칙:

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t-\bar{R_t})(1-\pi_t(A_t))
$$

$$
H_{t+1}(a) = H_t(a) - \alpha(R_t-\bar{R_t})\pi_t(a) \quad \forall a \neq A_t
$$

---

## 2. 유한 마르코프 결정 과정 (Finite Markov Decision Process, MDP)

**정의**  
어떤 action이 즉각적인 reward뿐 아니라 **미래의 state**에도 영향을 주어  
결국 장기적인 reward에 영향을 미치는 **연속적 의사결정 문제**.

즉, “지연된 보상(delayed reward)”을 포함.

---

### Agent–Environment Interface

- $S_t \in \mathcal{S}$ : 현재 상태  
- $A_t \in \mathcal{A}(s)$ : 선택한 행동  
- $R_{t+1} \in \mathcal{R}$ : 행동의 결과로 받은 보상  
- 다음 상태: $S_{t+1} \in \mathcal{S}$  

Trajectory:  
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, \dots$$

환경의 확률적 전이 모델:

$$
p(s',r|s,a) \mathrel{\dot{=}} \text{Pr}\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\}
$$

---

### 보상 정의

1. **상태 전이 확률**
   $$
   p(s'|s,a) = \sum_{r\in\mathcal{R}} p(s',r|s,a)
   $$

2. **보상의 기댓값**
   $$
   r(s,a) = \sum_{r\in\mathcal{R}} r \sum_{s'\in\mathcal{S}} p(s',r|s,a)
   $$

3. **특정 전이에 대한 보상**
   $$
   r(s,a,s') = \sum_{r\in\mathcal{R}} r \frac{p(s',r|s,a)}{p(s'|s,a)}
   $$

---

### 목표와 보상

**보상 가설 (Reward Hypothesis)**  
에이전트의 목표는 **받는 보상의 총합을 최대화**하는 것.

---

### 보상과 에피소드

- **Episodic task**  
  $$
  G_t = R_{t+1} + R_{t+2} + \dots + R_T
  $$  
  (종단 상태 T 존재)

- **Continuing task**  
  $$
  G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
  $$  
  (할인율 $\gamma$ 적용)

---

### 통합 표기법

$$
G_t = \sum_{k=t+1}^T \gamma^{k-t-1} R_k
$$

---

### 정책과 가치함수

- **정책(policy)**:  
  $$
  \pi(a|s) = \text{Pr}(A_t=a|S_t=s)
  $$

- **상태가치함수(state-value function)**:  
  $$
  v_\pi(s) = \mathbb{E}_\pi [\, G_t | S_t = s \,]
  = \mathbb{E}_\pi [\, R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s \,]
  $$

- **행동가치함수(action-value function)**:  
  $$
  q_\pi(s,a) = \mathbb{E}_\pi [\, G_t | S_t = s, A_t = a \,]
  = \mathbb{E}_\pi [\, R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \,]
  $$

---

### 벨만 방정식 (Bellman Equation)

$$
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[\,r + \gamma v_\pi(s')\,]
$$

---

### 최적 정책과 최적 가치함수

**강화학습의 목표**  
장기적으로 보상을 최대화하는 정책 $\pi_*$를 찾는 것.

$$
v_*(s) = \max_\pi v_\pi(s), \qquad
q_*(s,a) = \max_\pi q_\pi(s,a)
$$

---

### 최적 벨만 방정식

**상태 가치 형태**

$$
v_*(s) = \max_a \sum_{s',r} p(s',r|s,a)[\,r + \gamma v_*(s')\,]
$$

**행동 가치 형태**

$$
q_*(s,a) = \sum_{s',r} p(s',r|s,a)[\,r + \gamma \max_{a'} q_*(s',a')\,]
$$

---



## 8. 표에 기반한 방법을 이용한 계획 및 학습

### 모델과 계획 (Models and Planning)

**모델(Model)**이란 환경이 행동에 어떻게 반응할 것인지를 예측하기 위해 학습자가 사용할 수 있는 모든 것을 의미
모델은 상태와 행동이 주어지면, 그 결과로 나타날 수 있는 다음 상태와 보상을 예측

#### 1. 모델의 종류

- **분포 모델 (Distribution Models)**: 가능한 모든 다음 상태와 보상에 대해 각각이 발생할 확률을 포함하는 모델
- **표본 모델 (Sample Models)**: 확률 분포에 따라 추출된 하나의 가능한 다음 상태와 보상만을 생성하는 모델
- **비교**: 분포 모델은 표본 모델이 제공할 수 있는 모든 정보를 포함하지만, 12개의 주사위 합계 확률을 계산하는 것보다 한 번 주사위를 던지는 것이 쉽듯이 실제 구현에서는 표본 모델을 만드는 것이 훨씬 용이한 경우가 많음

#### 2. 계획 (Planning)

계획은 **모델을 입력으로 하여 정책을 도출하거나 향상시키는 모든 계산 과정**을 의미

- **상태 공간 계획 (State-space Planning)**: 최적 정책이나 목표를 향한 경로를 찾기 위해 상태 공간을 탐색하는 방식
- **계획의 공통 구조**: 모든 상태 공간 계획 방법은 다음과 같은 공통된 구조를 공유

  모델 → 시뮬레이션된 경험 → 보강(Backup) → 가치 → 정책

- **학습과의 통합**: 계획과 학습의 핵심은 가치 함수를 계산하는 것
- 둘 다 미래 사건을 내다보고 보강된(backed-up) 가치를 계산하여 이를 근사적 가치 함수의 갱신 목표로 사용한다는 점에서 유사

---

### 다이나(Dyna): 계획, 행동, 학습의 통합

다이나(Dyna)는 실제 경험을 사용하여 정책을 직접 향상시키는 **직접적 강화학습(Direct RL)**과, 실제 경험으로 모델을 배우고 그 모델에서 생성된 시뮬레이션 경험으로 정책을 향상시키는 **간접적 강화학습(Indirect RL, 즉 계획)**을 통합한 구조

#### 1. 직접적 강화학습과 간접적 강화학습의 상호작용

- **모델 학습 (Model-learning)**: 실제 경험을 통해 모델이 실제 환경과 더 유사해지도록 향상시키는 과정
- **직접적 강화학습 (Direct RL)**: 실제 경험을 통해 가치 함수와 정책을 직접 향상시키는 과정
- **계획 (Planning)**: 시뮬레이션된 경험을 통해 가치 함수와 정책을 향상시키는 과정

#### 2. 표 형태의 Dyna-Q 알고리즘 (Tabular Dyna-Q)

이 알고리즘은 실제 경험에서의 Q 학습과 모델을 통한 시뮬레이션 경험에서의 Q 학습을 병행

모든 $s \in \mathcal{S},\ a \in \mathcal{A}(s)$에 대해 $Q(s, a)$를 초기화하고, $Model(s, a)$를 빈 공간으로 초기화한 후 다음을 무한 루프:

1. $S \leftarrow$ 현재(비종단) 상태  
2. $A \leftarrow \epsilon\text{-greedy}(S, Q)$  
3. 행동 $A$를 취하고, 보상 $R$과 다음 상태 $S'$을 관측  
4. **직접적 RL (Q-학습)**:
   $$
   Q(S, A) \leftarrow Q(S, A) + \alpha \big[ R + \gamma \max_a Q(S', a) - Q(S, A) \big]
   $$
5. **모델 학습**:
   $$
   Model(S, A) \leftarrow R, S' \quad (\text{결정론적 환경 가정})
   $$
6. **계획 (Planning)**: 다음 과정을 $n$번 반복:
   - $S \leftarrow$ 이전에 관측된 임의의 상태  
   - $A \leftarrow$ 상태 $S$에서 이전에 취해진 임의의 행동  
   - $R, S' \leftarrow Model(S, A)$  
   - 
     $$
     Q(S, A) \leftarrow Q(S, A) + \alpha \big[ R + \gamma \max_a Q(S', a) - Q(S, A) \big]
     $$

#### 3. 계획 단계($n$)의 효과 (예제 8.1 다이나 미로)

- $n = 0$: 계획 단계가 없는 일반적인 Q 학습
- $n = 5,\ n = 50$: 계획 단계가 많을수록 학습자는 더 적은 에피소드 만에 목표에 도달하는 최적의 정책을 찾아냄
- 계획을 통해 한 번의 실제 경험이 여러 가치 함수의 갱신으로 이어지기 때문

---

### 모델이 틀렸을 때 (When the Model is Wrong)

모델은 환경이 변하거나, 경험이 부족하거나, 함수 근사 과정에서 오류가 발생할 때 실제 환경과 달라질 수 있음
모델이 부정확하면 계획 과정은 최적이 아닌 정책을 생성

#### 1. 차단 미로 (Blocking Maze)

환경이 이전보다 더 나쁜 방향으로 변한 경우

- 학습자는 바뀐 환경에서 벽을 마주치며 보상이 낮아짐을 경험
- 이 부정적인 경험이 모델에 반영되고 계획 과정을 통해 가치 함수가 수정되면서, 결국 새로운 우회로를 찾아냄

#### 2. 지름길 미로 (Shortcut Maze)

환경이 이전보다 더 좋은 방향으로 변한 경우

- **문제점**: 기존 모델 하에서 이미 최적의 경로를 찾았다면, 학습자는 새로운 지름길이 생긴 영역을 탐색하지 않을 수 있음
- 모델이 해당 경로를 '나쁜 것'으로 기억하고 있기 때문에 시뮬레이션에서도 탐색이 일어나지 않음
- 이 경우 단순한 Dyna-Q는 지름길을 발견하는 데 매우 오랜 시간이 걸리거나 영원히 발견하지 못할 수 있음

#### 3. 해결책: Dyna-Q+ (탐험 보너스)

학습자가 변화하는 환경을 더 잘 감지하도록 '계산적 호기심(Computational Curiosity)'을 추가

- 특정 상태-행동 쌍 $(S, A)$이 실제 환경에서 시도된 후 경과된 시간 단계를 $\tau$라고 함
- 계획 단계에서 모델이 제공하는 보상에 **보너스 보상**을 더함:
  $$
  R + \kappa \sqrt{\tau} \quad (\text{단, } \kappa \text{는 아주 작은 파라미터})
  $$
- 이 보너스는 오랫동안 시도되지 않은 행동을 계획 과정에서 더 가치 있게 평가하도록 만들어, 학습자가 실제 환경에서 해당 행동을 다시 시도하도록 유도

---

### 우선순위가 있는 일괄처리 (Prioritized Sweeping)

Dyna-Q의 계획 단계에서 상태-행동 쌍을 무작위로 선택하는 방식은 비효율적일 수 있음
가치가 바뀐 상태의 바로 앞 상태들은 가치가 바뀔 가능성이 높지만, 그 외의 많은 상태는 가치가 거의 변하지 않기 때문
**우선순위가 있는 일괄처리**는 가치 변화가 큰 곳에 계획의 노력을 집중하여 효율성을 높임

#### 1. 핵심 논리: 역방향 집중

- 상태의 가치가 크게 변하면, 그 상태로 이어지는 이전 상태들의 가치도 변할 가능성이 큼
- 가치 변화가 발생한 상태의 **앞선 상태(Predecessors)**들을 추적하여, 변화의 영향력을 역방향으로 전파

#### 2. 결정론적 환경에 대한 알고리즘 절차

모든 $s, a$에 대해 $Q(s, a)$와 $Model(s, a)$를 초기화하고, $PQueue$(우선순위 큐)를 빈 행렬로 초기화한 후 다음을 반복:

- (a) $S \leftarrow$ 현재 상태, $A \leftarrow policy(S, Q)$를 선택합
- (b) 행동 $A$를 취하고 보상 $R$과 다음 상태 $S'$을 관측
- (c) 모델 학습: $Model(S, A) \leftarrow R, S'$.
- (d) 갱신 우선순위 $P$ 계산:
  $$
  P \leftarrow \left| R + \gamma \max_a Q(S', a) - Q(S, A) \right|
  $$
- (e) 만약 $P > \theta$ (임계값)라면, $(S, A)$를 우선순위 $P$로 $PQueue$에 삽입
- (f) $PQueue$가 비어 있지 않은 동안 $n$번 반복:
  - $S, A \leftarrow first(PQueue)$ (가장 높은 우선순위를 가진 쌍을 추출).
  - $R, S' \leftarrow Model(S, A)$.
  - $Q$ 값 갱신:
    $$
    Q(S, A) \leftarrow Q(S, A) + \alpha \big[ R + \gamma \max_a Q(S', a) - Q(S, A) \big]
    $$
  - 현재 상태 $S$로 도달하는 것으로 예측되는 모든 이전 상태 $\bar{S}$와 행동 $\bar{A}$에 대해:
    - $\bar{R} \leftarrow$ $Model$이 $\bar{S}, \bar{A}$에 대해 예측하는 보상.
    - 
      $$
      \bar{P} \leftarrow \left| \bar{R} + \gamma \max_a Q(S, a) - Q(\bar{S}, \bar{A}) \right|
      $$
    - 만약 $\bar{P} > \theta$라면, $(\bar{S}, \bar{A})$를 우선순위 $\bar{P}$로 $PQueue$에 삽입

---

### 기댓값 갱신 대 표본 갱신 (Expected vs. Sample Updates)

계획 과정에서 가치 함수를 갱신할 때, 모델의 정보를 어떻게 활용하느냐에 따라 두 가지 방식으로 나뉨뉨

#### 1. 기댓값 갱신 (Expected Updates)

분포 모델을 사용하여 가능한 모든 다음 상태의 기댓값을 계산

- 수식 (8.1):
  $$
  Q(s, a) \leftarrow \sum_{s', r} \hat{p}(s', r \mid s, a)\big[ r + \gamma \max_{a'} Q(s', a') \big]
  $$
- 특징: 한 번의 갱신으로 정확한 기댓값을 얻지만, 분기 계수(Branching factor) $b$가 클수록 계산량이 $O(b)$로 증가

#### 2. 표본 갱신 (Sample Updates)

표본 모델로부터 얻은 단 하나의 표본 전이$(S', R)$만을 사용하여 점진적으로 갱신

- 수식 (8.2):
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \big[ R + \gamma \max_{a'} Q(S', a') - Q(s, a) \big]
  $$
- 특징: 계산량이 $O(1)$로 일정하며, 실제 경험을 통한 학습과 동일한 구조

#### 3. 계산 효율성 비교

- 분기 계수 $b$의 영향: $b$가 작을 때는 기댓값 갱신이 유리하지만, $b$가 매우 큰 경우에는 표본 갱신이 더 효율적일 수 있음
- 이유: 동일한 계산 시간 동안 표본 갱신은 기댓값 갱신보다 $b$배 더 많은 상태를 방문하여 갱신할 수 있기 때문
- 많은 상태에 대해 불완전한 표본 갱신을 수행하는 것이 소수의 상태에 대해 완벽한 기댓값 갱신을 하는 것보다 성능 향상에 더 유리한 경우가 많음

---

### 궤적 표본추출 (Trajectory Sampling)

갱신 작업을 상태 공간 전체에 어떻게 분산시킬 것인가에 대한 전략

#### 1. 균등 분포 (Uniform Distribution)

- 상태 공간 전체를 훑으며 모든 상태-행동 쌍을 동일하게 대우
- 단점: 상태 공간이 거대할 경우, 실제 정책 하에서 절대 마주치지 않을 상태들에까지 무의미한 계산 자원을 낭비하게 됨

#### 2. 궤적 표본추출 (Trajectory Sampling)

- 현재 정책 $\pi$를 따라 시뮬레이션된 궤적(Trajectory) 상에서 마주치는 상태들만 갱신
- 장점: 현재 정책 하에서 중요도가 높은(자주 방문하는) 상태들에 계산을 집중할 수 있음

#### 3. 성능 특징

- 초기 단계에서는 궤적 표본추출이 균등 분포보다 훨씬 빠르게 성능을 향상
- 현재 정책과 관련된 영역을 우선적으로 학습하기 때문
- 하지만 장기적으로는 이미 가치 추정이 정확해진 상태를 반복해서 갱신하게 되어, 최적 정책으로 완전히 수렴하는 속도는 균등 분포보다 느려질 수 있음
- 분기 계수 $b$가 작고 상태 공간이 클수록, 무관한 상태들을 걸러내는 궤적 표본추출의 이점이 커짐

---

### 실시간 동적 프로그래밍 (Real-Time Dynamic Programming, RTDP)

RTDP는 동적 프로그래밍(DP)의 가치 반복(Value Iteration) 알고리즘을 **활성 정책 궤적 표본추출(On-policy Trajectory Sampling)** 방식으로 구현한 것

#### 1. 핵심 개념과 특징

- **온라인 학습과의 관계**: 전통적인 DP와 밀접한 관련이 있지만, 실제 또는 시뮬레이션된 상호작용 중에 마주치는 상태들에 대해서만 가치를 갱신한다는 점에서 차이가 있음
- **관련 상태(Relevant States)에 집중**: 거대한 상태 공간을 가진 문제에서 모든 상태를 방문하는 것은 불가능. RTDP는 시작 상태 집합으로부터 최적 정책을 따랐을 때 도달 가능한 상태들, 즉 '관련 상태'에 대해서만 최적 정책을 찾는 데 집중
- **최적 부분 정책(Optimal Partial Policy)**: 모든 상태에 대해 최적 정책을 구하지 않더라도, 관련 상태들에 대해서만 최적의 행동을 결정할 수 있는 '부분적인 최적 정책'을 효율적으로 찾아냄

#### 2. 최적 정책 수렴 조건

RTDP가 모든 상태를 방문하지 않고도 관련 상태에 대해 최적 정책으로 수렴하기 위해서는 다음의 조건을 만족해야 함:

- **조건 1**: 모든 목표 상태의 초기 가치는 $0$
- **조건 2**: 에피소드는 항상 정해진 시작 상태 집합 내의 상태에서 시작
- **조건 3**: 관련된 모든 상태로부터 목표 상태에 도달함을 보장($100\%$ 확률)하는 최적 정책이 하나 이상 존재
- **조건 4**: 목표 상태가 아닌 상태에서 발생하는 모든 전이에 대한 보상은 $0$보다 작거나 같아야 함 ($R \le 0$).

#### 3. 성능 평가 (경주 트랙 예제)

- **DP와의 비교**: 전통적인 DP(일괄 처리)와 비교했을 때, RTDP는 훨씬 적은 수의 상태 갱신만으로도 최적에 가까운 성능을 낼 수 있음음
- **결과**: 책에 제시된 표에 따르면, 경주 트랙 문제에서 DP는 전체 상태의 100%를 수만 번 갱신해야 하지만, RTDP는 약 50% 미만의 상태만 갱신하고도 목표에 도달하는 정책을 찾아냈으며, 그 과정에서 갱신 횟수 또한 현저히 적음

---

### 결정 시점에서의 계획 (Planning at Decision Time)

계획이 수행되는 시점에 따라 '배경 계획'과 '결정 시점 계획'으로 구분할 수 있음

#### 1. 배경 계획 (Background Planning)

- **Dyna와 같은 방식**: 실제 경험이나 모델을 통해 얻은 시뮬레이션 경험을 바탕으로, 실제 환경과 상호작용하지 않는 '배경'에서 미리 가치 함수와 정책을 개선해 두는 방식
- **조회 방식**: 실제 상황이 닥치면 미리 계산해 둔 정책을 단순히 '조회'하여 행동을 선택

#### 2. 결정 시점 계획 (Decision-time Planning)

- **즉각적인 계산**: 특정 상태 $S_t$를 마주친 직후에 계획을 시작하여, 다음 행동 $A_t$를 선택하기 위한 계산을 즉석에서 수행하고 그 결과를 바로 사용
- **장점**: 상태 공간이 너무 커서 모든 상태에 대해 미리 정책을 준비하기 어려운 경우에 매우 유용
- **집중 탐색**: 현재 직면한 상태 $S_t$로부터 파생될 수 있는 가능한 미래 경로들에 대해서만 계산 자원을 집중
- **일회성 계산**: 대개 현재의 결정을 내리기 위해 수행된 방대한 계산 결과는 행동 선택 후 버려짐 하지만 이를 통해 얻은 가치 정보를 가치 함수나 정책을 업데이트하는 데 활용할 수도 있음음

---

### 경험적 탐색 (Heuristic Search)

경험적 탐색은 인공지능 분야에서 고전적인 상태 공간 계획 방법으로, '결정 시점 계획'의 가장 대표적인 형태

#### 1. 작동 방식

- **탐색 트리 확장**: 현재 상태를 Root 노드로 설정하고, 그로부터 가능한 미래의 상태들을 트리 형태로 확장
- **보강(Backup)**: 트리의 끝부분인 Leaf 노드에서 얻은 가치 추정치를 기반으로, 기댓값 갱신 방식을 사용하여 루트 노드(현재 상태) 방향으로 가치를 전달
- **행동 선택**: 루트 노드에서 가장 높은 보강 가치를 가진 행동을 Greedy로 선택

#### 2. 탐색의 깊이와 가치 함수

- **깊은 탐색**: 가치 함수가 부정확하거나 없는 경우, 더 정확한 결정을 내리기 위해 트리를 아주 깊게 확장
- **얕은 탐색**: 만약 이미 상당히 정확한 근사 가치 함수를 가지고 있다면, 트리를 깊게 확장할 필요 없이 얕은 탐색만으로도 훌륭한 행동을 선택할 수 있음 - '탐색을 얕게 만든다'
- **장점**: 탐색을 통해 여러 단계 앞을 내다봄으로써, 현재의 가치 함수가 가진 오류를 보정하고 더 나은 행동을 선택할 수 있게 해줌

#### 3. TD와의 관계

- 경험적 탐색은 TD 학습과 달리 가치 함수 자체를 영구적으로 개선하기보다는, '지금 당장의 결정을 더 잘 내리기 위해' 깊은 보강(Deep backups)을 수행하는 과정으로 이해할 수 있음

---

### 주사위 던지기 알고리즘 (Rollout Algorithms)

주사위 던지기 알고리즘은 현재 상태에서 시작하는 시뮬레이션 궤적들에 적용되는 결정 시점 계획 알고리즘

- **기본 원리**: 주어진 정책(주사위 던지기 정책)에 따라 현재 상태에서 가능한 각 행동을 취한 후, 많은 시뮬레이션 궤적을 생성하여 평균적인 보상의 합계를 계산
- **행동 선택**: 시뮬레이션 결과 가장 높은 기댓값을 갖는 행동을 실제 행동으로 선택하며, 선택이 완료된 후 그 과정에서 얻은 가치 추정치는 버려짐
- **정책 향상의 관점**: 주사위 던지기 알고리즘의 목적은 최적 정책을 찾는 것이 아니라, 시뮬레이션에 사용된 기본 주사위 던지기 정책보다 더 좋은 행동을 선택하는 정책 향상에 있음
- **특징**:
  - 전통적인 동적 프로그래밍과 달리 가치 함수의 전체적인 최적 해를 구하려 하지 않고 현재 상태에만 집중
  - 성능은 충분히 많은 시뮬레이션을 수행할 시간적 여유와 주사위 던지기 정책의 품질에 좌우
  - 각 시뮬레이션 시행은 서로 독립적이므로 병렬 처리에 매우 적합

---

### 몬테카를로 트리 탐색 (Monte Carlo Tree Search, MCTS)

MCTS는 주사위 던지기 알고리즘의 한 형태로, 시뮬레이션 결과를 바탕으로 탐색 트리를 점진적으로 확장하여 가치 추정치를 축적하는 고도화된 결정 시점 계획법

#### 1. MCTS의 4단계 반복 과정

MCTS는 다음의 네 단계를 반복하여 수행하며, 시간이 허락하는 한 많은 시뮬레이션을 실행

1. **선택 (Selection)**: 루트 노드(현재 상태)에서 시작하여 **트리 정책(tree policy)**에 따라 이미 확장된 트리 내에서 유망한 리프 노드에 도달할 때까지 아래로 이동
2. **확장 (Expansion)**: 도달한 리프 노드에서 아직 시도되지 않은 행동이 있다면, 하나 또는 그 이상의 자식 노드를 추가하여 트리를 확장
3. **시뮬레이션 (Simulation)**: 새롭게 추가된 노드 또는 선택된 노드에서 **주사위 던지기 정책(rollout policy)**에 따라 에피소드가 끝날 때까지 시뮬레이션을 수행
4. **역전파 (Backpropagation)**: 시뮬레이션 끝에 얻은 보상 결과를 해당 경로상의 모든 트리 노드에 전달하여 가치 추정치(방문 횟수 및 평균 보상)를 갱신

#### 2. 주요 특징과 성공 요인

- **트리 정책**: 탐험(Exploration)과 활용(Exploitation)의 균형을 맞추기 위해 입실론 탐욕적 규칙이나 UCB(Upper Confidence Bound) 등을 사용
- **선택적 확장**: 가치가 높은 경로를 집중적으로 탐색하므로, 거대한 상태 공간에서도 효율적으로 유망한 행동을 찾아낼 수 있음
- **응용**: 바둑(AlphaGo)이나 체스 같은 복잡한 게임에서 압도적인 성능을 증명하며 널리 알려잠

---

