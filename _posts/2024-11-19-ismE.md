---
title: "Financial Data Analysis for Time Series Forecasting — FTS-Diffusion 노트"
date: 2025-10-28 12:00:00 +0900
categories: [notes, financial_time_series, diffusion_models]
tags: [FTS-Diffusion, scale-invariance, irregularity, time-series, DTW, SISC, KMeans++, diffusion, DDPM, AE, Markov, augmentation]
math: true
permalink: /notes/financial-time-series/fts-diffusion/
---

# Seminar Notes — *Financial Data Analysis for Time Series Forecasting* (FTS-Diffusion)

## Target Paper
[1] Huang, Hongbin, Minghua Chen, and Xiao Qiao. “Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns.” *ICLR* 2024.  
[2] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. “Denoising Diffusion Probabilistic Models.” *NeurIPS* 2020.  
[3] Sohl-Dickstein, Jascha, et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” *ICML* 2015.

---

## Introduction

금융 시계열 학습에서 **데이터 부족**이 핵심 문제. 2024 OIBC(제주 전력 하루전시장 가격)도 데이터가 8개월 수준. 데이터가 충분해질 즈음엔 가격이 **안정화**되어 ML/DL 이점이 약해짐. 금융 시계열은 실험으로 **추가 수집이 어렵다**는 특성도 있다.

대안으로 **데이터 증강**을 쓰지만, 금융 도메인의 **Scale-Invariance**(기간·크기 스케일이 달라도 개형은 유사)와 **Irregularity**(재등장 시점 불규칙) 때문에 일반 증강으로는 한계.  
- **Scale-Invariance:** 다양한 기간/진폭의 패턴이 **소수 개형**으로 환원 가능. 동일 개형이 짧게/작게 또는 길게/크게 나타남. 제조 신호 등 **Scale-Dependence**와 대비.  
- **Irregularity:** 특정 패턴이 **언제 다시 나타날지 모름**. 주기/규칙이 약함. 주파수 분석 기반 정형 시계열과 대비.

논문은 두 특성을 반영해 **3-모듈 FTS-Diffusion 프레임워크** 제안:
1) **Pattern Recognition** — 핵심 개형과 크기 추출(전처리)  
2) **Pattern Generation** — 개형 조건 분포로 패턴 생성(DDPM+AE)  
3) **Pattern Evolution** — 개형/스케일의 **순차 전개**(마코프 체인; ‘개구리 점프’)

---

## Pattern Recognition Module (SISC)

목표: 전체 시계열 $X_{1:T}$를 $m$개 **segment**로 나누고, 각 segment를 **$K$개 대표 개형**에 할당.

### 1) 중심 초기화(K-Means++ 유사)
- 길이 상한 $\ell_{\max}$ 설정. 모든 시작점 $t$에 대해 후보 창 $X_{t:t+\ell_{\max}}$ 생성(총 $T-\ell_{\max}+1$개).
- 첫 중심 $p_1$ 무작위 선택. 이후는 **현재 중심들과 가장 먼** 후보가 뽑힐 확률이 높도록 샘플링해 $p_2,\dots,p_K$ 선택.
- 초기 중심 집합 $\mathcal{P}=\{p_1,\dots,p_K\}$.

### 2) 최적 길이 추정 + 분할
- 현재 패턴 시작 $t$에서 segment 길이 $\ell$와 대표 개형 $p\in\mathcal{P}$를 탐색:
  $$
  \ell_m^* \;=\; \arg\min_{\ell\in[\ell_{\min},\ell_{\max}],\; p\in\mathcal{P}}
  d\!\big(X_{t:t+\ell},\; p\big).
  $$
- 거리 $d(\cdot,\cdot)$는 **DTW**. 단, DTW 전에 **정규화**:
  - 기간 스케일 $\alpha_m=\ell_m^*$  
  - 값 스케일 $\beta_m=\max(s_m)-\min(s_m)$  
  - $s_m := X_{t:t+\ell_m^*}$를 $(\alpha_m,\beta_m)$로 정규화 → **개형(모양)만** 비교.

> DTW는 시간 축을 뒤틀어 두 시계열 정렬 거리를 최소화. 유클리디안과 달리 **속도/길이 차이**에 강건.

### 3) 군집화 반복(K-Means)
- 각 $s_m$을 최근접 대표 개형 $p_k$에 할당, 중심 업데이트 → 수렴까지 반복.  
- 최종 표현:  
  $$
  s_m \;\equiv\; \{\,p_k,\ \alpha_m,\ \beta_m,\ \text{normalized}(s_m)\,\}.
  $$

---

## Pattern Generation Module (개형 조건 생성)

아이디어: **개형은 핵심 정보**. 개형 군집이 정해지면, 해당 군집의 **개형 분포를 생성**하고, 별도로 **기간/크기 스케일**을 복원.

### 구성
- **AE(오토인코더)**  
  - Encoder: segment $s_m$ 입력 → **정규화된 개형** $s_{0,m}$ 출력(기간·크기 제거).  
  - Decoder: $(\tilde s_{0,m},\alpha_m,\beta_m)$ 입력 → **원 스케일** segment 복원 $\hat s_m$.
- **DDPM(개형 생성기)**  
  - 입력: $s_{0,m}$ 조건.  
  - 학습: 노이즈를 추가/제거하며 **개형 분포**를 모델링(조건부 생성).

### 손실
- 복원 오차(Decoder 중심) + 확산 오차(DDPM 중심):
  $$
  \mathcal{L}
  \;=\;
  \underbrace{\sum_m \big\|\mathrm{Dec}(\tilde s_{0,m},\alpha_m,\beta_m) - s_m\big\|_2^2}_{\mathcal{L}_{\text{recon}}}
  \;+\;
  \lambda\;\underbrace{\mathbb{E}_{t,\epsilon}\big[\|\epsilon - \epsilon_\theta(s_{0,m},t)\|_2^2\big]}_{\mathcal{L}_{\text{DDPM}}}.
  $$

---

## DDPM Primer (요약)

**정방향(노이즈 추가)**  
$$
q(x_t\mid x_{t-1}) \;=\; \mathcal{N}\!\big(\sqrt{\alpha_t}\,x_{t-1},\ (1-\alpha_t)\mathbf{I}\big),\quad
\alpha_t=1-\beta_t.
$$
닫힌형:
$$
q(x_t\mid x_0) \;=\; \mathcal{N}\!\big(\sqrt{\bar\alpha_t}\,x_0,\ (1-\bar\alpha_t)\mathbf{I}\big),
\quad \bar\alpha_t=\prod_{i=1}^t\alpha_i.
$$

**역방향(복원; 학습 모델)**  
$$
p_\theta(x_{t-1}\mid x_t)\approx
\mathcal{N}\!\Big(\mu_\theta(x_t,t),\ \sigma_t^2\mathbf{I}\Big),\quad
\mu_\theta(x_t,t)=\tfrac{1}{\sqrt{\alpha_t}}\!\Big(x_t-\tfrac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\,
\epsilon_\theta(x_t,t)\Big).
$$

**학습 목표(MSE)**  
$$
\min_\theta\ \mathbb{E}_{x_0,t,\epsilon}\big[\|\epsilon-\epsilon_\theta(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon,\ t)\|_2^2\big].
$$

---

## Pattern Evolution Module (개구리 점프; 마코프)

목표: 다음 패턴의 **개형/기간/크기**를 **직전 패턴만** 보고 샘플링.  
상태 $z_m=(p_m,\alpha_m,\beta_m)$에 대해
$$
Q_\phi(z_{m+1}\mid z_m)
\;=\;
Q_\phi\big(p_{m+1}\mid z_m\big)\;
Q_\phi\big(\alpha_{m+1}\mid z_m\big)\;
Q_\phi\big(\beta_{m+1}\mid z_m\big).
$$

- $p_{m+1}$: $K$-분류(소프트맥스)  
- $\alpha_{m+1},\beta_{m+1}$: 회귀

**학습 손실(예시)**
$$
\mathcal{L}_{\text{evo}}
=
\mathrm{CE}\big(p_{m+1},\ \pi_\phi(z_m)\big)
+\eta\,\|\alpha_{m+1}-\hat\alpha_\phi(z_m)\|_2^2
+\zeta\,\|\beta_{m+1}-\hat\beta_\phi(z_m)\|_2^2.
$$

샘플링은 $z_m\!\to\!z_{m+1}\!\to\!z_{m+2}\!\to\cdots$ 순차 점프. 이렇게 얻은 $(p,\alpha,\beta)$ 시퀀스를 **Pattern Generation**에 넣어 실제 segment를 생성. 연속성과 일관성 유지.

---

## 전체 파이프라인

1) **SISC**로 $X$를 $[\,\{p,\alpha,\beta,\text{normalized}(s)\,\}_m\,]$로 분해.  
2) **DDPM+AE**로 개형 분포 학습 → 조건부 생성(개형) → Decoder로 스케일 복원.  
3) **Evolution(마코프)**로 다음 $(p,\alpha,\beta)$ 샘플링 → 2)에 입력 → 시계열 생성.

---

## 한계와 코멘트

- 과업은 **현실 유사 데이터 증강**. 실험 비교에서, 다른 생성 기반 증강은 **데이터가 많아질수록 과적합** 경향. 제안법은 과적합은 덜하지만, **예측 성능 향상은 제한적**.  
- 해석: 생성 데이터가 **불규칙 패턴**을 담아 모델의 **노이즈 강건성**은 높였으나, **현실 분포 근접**까지는 미흡.  
- 결론: **예측 성능 향상용** 증강으로는 의문. 다만 **리스크 시나리오** 생성이나 **스트레스 테스트**, **로버스트 학습** 용도로는 가치 있음.

---

## 부록: SISC 세부 수식(요약)

- 길이 선택:
$$
\ell_m^*=\arg\min_{\ell\in[\ell_{\min},\ell_{\max}],\,p\in\mathcal{P}}
\operatorname{DTW}\!\left(\operatorname{normalize}(X_{t:t+\ell};\ell,\text{range}),\ p\right).
$$

- 스케일:
$$
\alpha_m=\ell_m^*,\qquad \beta_m=\max(s_m)-\min(s_m).
$$

- segment 표현:
$$
s_m=\big\{p_k,\ \alpha_m,\ \beta_m,\ \text{normalized}(s_m)\big\}.
$$
